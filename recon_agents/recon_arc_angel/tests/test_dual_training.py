"""
Test dual training: CNN (StochasticGoose) + ResNet (BlindSquirrel)
"""
import pytest
import sys
import os
import torch
import numpy as np

# Add parent directories to path
sys.path.insert(0, "/workspace/recon-platform")

from recon_engine.neural_terminal import CNNValidActionTerminal, ResNetActionValueTerminal

def test_cnn_training_is_working():
    """Verify that CNN training is working (StochasticGoose style)"""
    from recon_agents.recon_arc_angel.learning_manager import LearningManager
    
    learning_manager = LearningManager(buffer_size=100, batch_size=5, train_frequency=3)
    
    # Create CNN terminal
    cnn_terminal = CNNValidActionTerminal("test_cnn", use_gpu=True)
    learning_manager.set_cnn_terminal(cnn_terminal)
    
    # Add diverse experiences (avoid deduplication)
    for i in range(6):
        frame1 = torch.zeros(16, 64, 64)
        frame1[1, i, i] = 1.0  # Different pattern each time
        
        frame2 = torch.zeros(16, 64, 64)
        frame2[2, i, i] = 1.0  # Different result each time
        
        learning_manager.add_experience(frame1, frame2, "action_1")
        learning_manager.step()
        
        # Manually trigger training when conditions are met
        if learning_manager.should_train():
            metrics = learning_manager.train_step()
            print(f"‚úÖ CNN training triggered: {metrics}")
    
    # Check that training occurred
    stats = learning_manager.get_stats()
    assert stats['total_training_steps'] > 0, f"CNN should have been trained, got {stats['total_training_steps']}"
    
    print(f"‚úÖ CNN training steps: {stats['total_training_steps']}")

def test_resnet_training_missing():
    """Test that ResNet training is currently missing"""
    
    # Create ResNet terminal
    resnet_terminal = ResNetActionValueTerminal("test_resnet")
    
    # Check if it has training capability
    has_optimizer = hasattr(resnet_terminal, 'optimizer')
    has_train_method = hasattr(resnet_terminal, 'train_step')
    
    print(f"ResNet has optimizer: {has_optimizer}")
    print(f"ResNet has train_step: {has_train_method}")
    
    # Currently missing - this is what we need to implement
    assert not has_train_method, "ResNet training not yet implemented"

class StateGraph:
    """Simplified state graph for ResNet training (BlindSquirrel style)"""
    
    def __init__(self):
        self.states = {}  # (game_id, score, frame_hash) -> state_info
        self.transitions = {}  # (state, action) -> next_state
        self.milestones = {}  # (game_id, score) -> winning_state
    
    def add_state(self, game_id: str, score: int, frame: torch.Tensor):
        """Add state to graph"""
        frame_hash = hash(frame.cpu().numpy().tobytes())
        state_key = (game_id, score, frame_hash)
        
        if state_key not in self.states:
            self.states[state_key] = {
                'game_id': game_id,
                'score': score,
                'frame': frame,
                'frame_hash': frame_hash
            }
        
        return state_key
    
    def add_transition(self, from_state, action_type, coords, to_state):
        """Add transition between states"""
        transition_key = (from_state, action_type, coords)
        self.transitions[transition_key] = to_state
    
    def add_milestone(self, game_id: str, score: int, state_key):
        """Add milestone (level completion)"""
        self.milestones[(game_id, score)] = state_key
    
    def compute_distances_to_goal(self, game_id: str, target_score: int):
        """Compute distance from each state to goal (BlindSquirrel approach)"""
        if (game_id, target_score) not in self.milestones:
            return {}
        
        goal_state = self.milestones[(game_id, target_score)]
        distances = {goal_state: 0}
        
        # Simple BFS backward from goal
        queue = [goal_state]
        visited = {goal_state}
        
        while queue:
            current_state = queue.pop(0)
            current_distance = distances[current_state]
            
            # Find all transitions that lead to current_state
            for (from_state, action_type, coords), to_state in self.transitions.items():
                if to_state == current_state and from_state not in visited:
                    distances[from_state] = current_distance + 1
                    queue.append(from_state)
                    visited.add(from_state)
        
        return distances

class ResNetTrainingManager:
    """Manages ResNet training using BlindSquirrel's approach"""
    
    def __init__(self, resnet_terminal):
        self.resnet_terminal = resnet_terminal
        self.state_graph = StateGraph()
        self.current_game_id = None
        self.current_score = -1
        
        # Training parameters (from BlindSquirrel)
        self.learning_rate = 1e-4
        self.batch_size = 32
        self.num_epochs = 10
        self.max_train_time = 15 * 60  # 15 minutes
        
        # Create optimizer
        if torch.cuda.is_available():
            self.resnet_terminal.to_device('cuda')
        
        self.optimizer = torch.optim.Adam(
            self.resnet_terminal.model.parameters(), 
            lr=self.learning_rate
        )
        self.criterion = torch.nn.MSELoss()
    
    def add_state_transition(self, prev_frame, action_type, coords, curr_frame, game_id, score):
        """Add state transition to graph"""
        # Add states
        prev_state = self.state_graph.add_state(game_id, score, prev_frame)
        curr_state = self.state_graph.add_state(game_id, score, curr_frame)
        
        # Add transition
        self.state_graph.add_transition(prev_state, action_type, coords, curr_state)
        
        # Check for score increase (milestone)
        if score > self.current_score:
            self.state_graph.add_milestone(game_id, score, curr_state)
            self.current_score = score
            
            # Trigger training on score increase (BlindSquirrel style)
            if score > 0:  # Don't train on first level
                self.train_resnet_on_level_completion(game_id, score)
    
    def train_resnet_on_level_completion(self, game_id: str, completed_score: int):
        """Train ResNet when level is completed (BlindSquirrel approach)"""
        print(f"üéì Training ResNet for {game_id} level {completed_score}")
        
        # Get training data from state graph
        training_data = self._prepare_training_data(game_id, completed_score)
        
        if len(training_data) < self.batch_size:
            print(f"‚ö†Ô∏è  Not enough data for training: {len(training_data)} < {self.batch_size}")
            return
        
        # Train ResNet
        self.resnet_terminal.model.train()
        
        import time
        start_time = time.time()
        
        for epoch in range(self.num_epochs):
            if time.time() - start_time > self.max_train_time:
                print(f"‚ö†Ô∏è  Reached max training time: {self.max_train_time}s")
                break
            
            # Sample batch
            batch_indices = np.random.choice(len(training_data), self.batch_size, replace=False)
            batch = [training_data[i] for i in batch_indices]
            
            # Prepare batch
            states = torch.stack([item['state'] for item in batch])
            actions = torch.stack([item['action'] for item in batch])
            values = torch.stack([item['value'] for item in batch])
            
            device = next(self.resnet_terminal.model.parameters()).device
            states = states.to(device)
            actions = actions.to(device)
            values = values.to(device)
            
            # Forward pass
            self.optimizer.zero_grad()
            predictions = self.resnet_terminal.model(states, actions)
            loss = self.criterion(predictions.squeeze(), values)
            
            # Backward pass
            loss.backward()
            self.optimizer.step()
            
            if epoch % 2 == 0:
                print(f"  Epoch {epoch+1}/{self.num_epochs}: Loss {loss.item():.4f}")
        
        self.resnet_terminal.model.eval()
        print(f"‚úÖ ResNet training completed for level {completed_score}")
    
    def _prepare_training_data(self, game_id: str, target_score: int):
        """Prepare training data using distance-to-goal (BlindSquirrel approach)"""
        distances = self.state_graph.compute_distances_to_goal(game_id, target_score)
        
        training_data = []
        max_distance = max(distances.values()) if distances else 1
        
        for (from_state, action_type, coords), to_state in self.state_graph.transitions.items():
            if from_state in distances and to_state in distances:
                from_distance = distances[from_state]
                to_distance = distances[to_state]
                
                # Value = progress toward goal (positive if getting closer)
                value = (from_distance - to_distance) / max_distance
                
                # Get state and action tensors
                state_info = self.state_graph.states[from_state]
                state_tensor = state_info['frame']
                
                # Create action tensor (simplified for testing)
                action_tensor = self._create_action_tensor(action_type, coords)
                
                training_data.append({
                    'state': state_tensor,
                    'action': action_tensor,
                    'value': torch.tensor(value, dtype=torch.float32)
                })
        
        return training_data
    
    def _create_action_tensor(self, action_type: str, coords):
        """Create action tensor for ResNet input"""
        # Simplified action encoding for testing
        action_tensor = torch.zeros(26)  # BlindSquirrel uses 26-dim action embedding
        
        if action_type.startswith("action_") and action_type != "action_click":
            # Individual action
            action_num = int(action_type.split("_")[1]) - 1
            action_tensor[action_num] = 1.0
        elif action_type == "action_click" and coords is not None:
            # Click action with coordinates
            action_tensor[5] = 1.0  # ACTION6
            y, x = coords
            action_tensor[6] = y / 64.0  # Normalized y
            action_tensor[7] = x / 64.0  # Normalized x
        
        return action_tensor

def test_state_graph():
    """Test the state graph for ResNet training"""
    graph = StateGraph()
    
    # Create test states
    frame1 = torch.zeros(16, 64, 64)
    frame2 = torch.zeros(16, 64, 64)
    frame2[1, 0, 0] = 1.0
    
    state1 = graph.add_state("test_game", 0, frame1)
    state2 = graph.add_state("test_game", 1, frame2)  # Score increased
    
    # Add transition
    graph.add_transition(state1, "action_1", None, state2)
    
    # Add milestone
    graph.add_milestone("test_game", 1, state2)
    
    # Compute distances
    distances = graph.compute_distances_to_goal("test_game", 1)
    
    assert state2 in distances
    assert distances[state2] == 0  # Goal state
    
    if state1 in distances:
        assert distances[state1] == 1  # One step from goal
    
    print(f"‚úÖ State graph working: {len(distances)} states with distances")

def test_resnet_training_manager():
    """Test ResNet training manager"""
    
    # Create ResNet terminal
    resnet_terminal = ResNetActionValueTerminal("test_resnet")
    training_manager = ResNetTrainingManager(resnet_terminal)
    
    # Simulate state transitions
    frame1 = torch.zeros(16, 64, 64)
    frame2 = torch.zeros(16, 64, 64)
    frame2[1, 10, 10] = 1.0
    frame3 = torch.zeros(16, 64, 64)
    frame3[2, 20, 20] = 1.0  # Score increase
    
    # Add transitions
    training_manager.add_state_transition(frame1, "action_1", None, frame2, "test_game", 0)
    training_manager.add_state_transition(frame2, "action_click", (20, 20), frame3, "test_game", 1)
    
    # Check that training was triggered on score increase
    print("‚úÖ ResNet training manager created and tested")

def test_dual_training_integration():
    """Test that both CNN and ResNet can be trained together"""
    from recon_agents.recon_arc_angel.learning_manager import LearningManager
    
    # Create both terminals
    cnn_terminal = CNNValidActionTerminal("test_cnn", use_gpu=True)
    resnet_terminal = ResNetActionValueTerminal("test_resnet")
    
    # Create learning manager for CNN
    cnn_learning = LearningManager(buffer_size=50, batch_size=5, train_frequency=2)
    cnn_learning.set_cnn_terminal(cnn_terminal)
    
    # Create ResNet training manager
    resnet_training = ResNetTrainingManager(resnet_terminal)
    
    # Simulate training scenario
    frame1 = torch.zeros(16, 64, 64)
    frame2 = torch.zeros(16, 64, 64)
    frame2[1, 5, 5] = 1.0
    
    # Add experience to CNN training
    cnn_learning.add_experience(frame1, frame2, "action_1")
    cnn_learning.step()
    
    # Add transition to ResNet training
    resnet_training.add_state_transition(frame1, "action_1", None, frame2, "test_game", 0)
    
    print("‚úÖ Dual training setup working")
    print(f"‚úÖ CNN experiences: {len(cnn_learning.experience_buffer)}")
    print(f"‚úÖ ResNet states: {len(resnet_training.state_graph.states)}")

def test_training_trigger_conditions():
    """Test when training should be triggered"""
    from recon_agents.recon_arc_angel.learning_manager import LearningManager
    
    # Test that training trigger logic works conceptually
    learning_manager = LearningManager(batch_size=3, train_frequency=5)
    
    # Should not train without enough experiences
    assert not learning_manager.should_train()
    
    # Should not train without CNN terminal
    learning_manager.action_count = 5  # Multiple of frequency
    assert not learning_manager.should_train()  # No CNN terminal
    
    # Should train with CNN terminal and enough data
    cnn_terminal = CNNValidActionTerminal("test_cnn", use_gpu=True)
    learning_manager.set_cnn_terminal(cnn_terminal)
    
    # Manually add experiences to buffer
    for i in range(3):
        learning_manager.experience_buffer.append({'test': i})
    
    assert learning_manager.should_train()  # Now has CNN, data, and right action_count
    
    print("‚úÖ Training trigger logic works correctly")
